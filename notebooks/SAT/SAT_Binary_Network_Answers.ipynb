{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iclimbtrees/iclimbtrees/blob/main/notebooks/SAT/SAT_Binary_Network_Answers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"data:image/svg+xml;base64,<?xml version="1.0" encoding="UTF-8"?>
<svg width="764.45" height="63.759" version="1.1" viewBox="0 0 764.45 63.759" xmlns="http://www.w3.org/2000/svg">
 <g transform="matrix(.73548 0 0 .73548 0 3.388)" stroke-width="1.3597">
  <rect x="5e-7" y="5e-7" width="77.387" height="77.387" ry="11.35" fill="#4469d8" opacity=".98" stroke-width="0"/>
  <text x="10.330567" y="65.761719" fill="#ffffff" font-family="Courier" font-size="93.483px" stroke-width="1.3597" style="line-height:1.25" xml:space="preserve"><tspan x="10.330567" y="65.761719" fill="#ffffff" font-family="Courier" stroke-width="1.3597">I </tspan></text>
  <rect x="96" y="5e-7" width="77.387" height="77.387" ry="11.35" fill="#4469d8" opacity=".98" stroke-width="0"/>
  <text x="106.33057" y="65.761719" fill="#ffffff" font-family="Courier" font-size="93.483px" stroke-width="1.3597" style="line-height:1.25" xml:space="preserve"><tspan x="106.33057" y="65.761719" fill="#ffffff" font-family="Courier" stroke-width="1.3597">C </tspan></text>
  <rect x="180" y="5e-7" width="77.387" height="77.387" ry="11.35" fill="#4469d8" opacity=".98" stroke-width="0"/>
  <text x="190.33057" y="65.761719" fill="#ffffff" font-family="Courier" font-size="93.483px" stroke-width="1.3597" style="line-height:1.25" xml:space="preserve"><tspan x="190.33057" y="65.761719" fill="#ffffff" font-family="Courier" stroke-width="1.3597">L </tspan></text>
  <rect x="264" y="5e-7" width="77.387" height="77.387" ry="11.35" fill="#4469d8" opacity=".98" stroke-width="0"/>
  <text x="274.33057" y="65.761719" fill="#ffffff" font-family="Courier" font-size="93.483px" stroke-width="1.3597" style="line-height:1.25" xml:space="preserve"><tspan x="274.33057" y="65.761719" fill="#ffffff" font-family="Courier" stroke-width="1.3597">I </tspan></text>
  <rect x="348" y="5e-7" width="77.387" height="77.387" ry="11.35" fill="#4469d8" opacity=".98" stroke-width="0"/>
  <text x="358.33057" y="65.761719" fill="#ffffff" font-family="Courier" font-size="93.483px" stroke-width="1.3597" style="line-height:1.25" xml:space="preserve"><tspan x="358.33057" y="65.761719" fill="#ffffff" font-family="Courier" stroke-width="1.3597">M </tspan></text>
  <rect x="432" y="5e-7" width="77.387" height="77.387" ry="11.35" fill="#4469d8" opacity=".98" stroke-width="0"/>
  <text x="442.33057" y="65.761719" fill="#ffffff" font-family="Courier" font-size="93.483px" stroke-width="1.3597" style="line-height:1.25" xml:space="preserve"><tspan x="442.33057" y="65.761719" fill="#ffffff" font-family="Courier" stroke-width="1.3597">B </tspan></text>
  <g transform="translate(2 1.5376)">
   <rect x="624" y="-1.5376" width="77.387" height="77.387" ry="11.35" fill="#4469d8" opacity=".98" stroke-width="0"/>
   <text x="634.33057" y="64.224167" fill="#ffffff" font-family="Courier" font-size="93.483px" stroke-width="1.3597" style="line-height:1.25" xml:space="preserve"><tspan x="634.33057" y="64.224167" fill="#ffffff" font-family="Courier" stroke-width="1.3597">T </tspan></text>
   <rect x="708" y="-1.5376" width="77.387" height="77.387" ry="11.35" fill="#4469d8" opacity=".98" stroke-width="0"/>
   <text x="718.33057" y="64.224167" fill="#ffffff" font-family="Courier" font-size="93.483px" stroke-width="1.3597" style="line-height:1.25" xml:space="preserve"><tspan x="718.33057" y="64.224167" fill="#ffffff" font-family="Courier" stroke-width="1.3597">R </tspan></text>
   <rect x="792" y="-1.5376" width="77.387" height="77.387" ry="11.35" fill="#4469d8" opacity=".98" stroke-width="0"/>
   <text x="802.33057" y="64.224167" fill="#ffffff" font-family="Courier" font-size="93.483px" stroke-width="1.3597" style="line-height:1.25" xml:space="preserve"><tspan x="802.33057" y="64.224167" fill="#ffffff" font-family="Courier" stroke-width="1.3597">E</tspan></text>
   <rect x="876" y="-1.5376" width="77.387" height="77.387" ry="11.35" fill="#4469d8" opacity=".98" stroke-width="0"/>
   <text x="886.33057" y="64.224167" fill="#ffffff" font-family="Courier" font-size="93.483px" stroke-width="1.3597" style="line-height:1.25" xml:space="preserve"><tspan x="886.33057" y="64.224167" fill="#ffffff" font-family="Courier" stroke-width="1.3597">E </tspan></text>
   <rect x="960" y="-1.5376" width="77.387" height="77.387" ry="11.35" fill="#4469d8" opacity=".98" stroke-width="0"/>
   <text x="970.33057" y="64.224167" fill="#ffffff" font-family="Courier" font-size="93.483px" stroke-width="1.3597" style="line-height:1.25" xml:space="preserve"><tspan x="970.33057" y="64.224167" fill="#ffffff" font-family="Courier" stroke-width="1.3597">S </tspan></text>
  </g>
  <g transform="matrix(1.0499 0 0 1.0499 -28.092 -.27293)" fill="#4469d8" stroke="#fdffff">
   <rect x="528" y="-1.5376" width="77.387" height="77.387" ry="11.35" opacity=".98" stroke-width="5.18"/>
   <g transform="matrix(.74592 0 0 .74367 530.84 1.6744)" stroke-width="5.2162" featureKey="inlineSymbolFeature-0">
    <g fill="#4469d8" stroke="#fdffff" stroke-width="5.2162">
     <g fill="#4469d8" stroke="#fdffff" stroke-width="5.2162">
      <path d="m47.659 81.427c0.358-7.981 1.333-15.917 1.152-23.917-0.01-0.425-0.544-0.843-0.94-0.54-2.356 1.801-4.811 3.219-7.664 4.104-3.649 1.132-7.703-2.328-5.814-5.981 0.758-1.466 2.146-2.708 3.447-3.672 0.467-0.346 0.358-1.176-0.315-1.165-3.154 0.054-10.835 1.149-10.042-4.386 0.481-3.365 6.29-5.458 8.917-6.84 0.333-0.175 0.435-0.73 0.127-0.981-6.663-5.431-3.069-14.647 5.731-12.788 0.272 0.058 0.563-0.033 0.706-0.287 2.235-3.995 4.276-8.063 7.106-11.688-0.356-0.147-0.712-0.294-1.067-0.442 0.294 3.116 2.036 5.269 4.337 7.272 2.459 2.142 7.634 4.27 8.085 7.845 0.481 3.821-6.549 4.356-6.054 7.588 0.33 2.147 1.354 3.423 3.021 4.74 1.052 0.831 1.968 1.405 3.017 2.329 1.818 2.036 1.596 4.223-0.667 6.561-1.486 0.252-2.927 0.138-4.32-0.341-0.556-0.144-0.945 0.435-0.706 0.918 1.412 2.842 3.23 5.449 3.529 8.707 0.821 8.969-7.237 1.748-8.13 0.875-0.813-0.793-1.6-1.561-2.486-2.27-0.623-0.498-1.514 0.38-0.885 0.884 3.399 2.717 6.507 7.782 11.132 4.42 4.323-3.142-0.524-10.114-2.08-13.246-0.235 0.306-0.471 0.612-0.706 0.918 3.9 1.01 8.231 0.447 7.941-4.452-0.117-1.973-1.259-3.644-2.8-4.778-1.468-1.081-6.729-4.234-3.68-6.41 1.261-0.899 2.453-1.826 3.548-2.929 2.294-2.311 1.726-4.94-0.326-7.105-3.535-3.732-9.97-5.682-10.521-11.525-0.044-0.47-0.692-0.921-1.067-0.442-1.267 1.622-6.265 11.724-7.841 11.391-2.234-0.472-4.485 0.06-6.418 1.186-4.105 2.391-3.919 7.903-1.738 11.448 0.122 0.199 1.517 2.084 1.782 1.944-1.682 0.885-3.351 1.737-4.951 2.768-1.664 1.072-4.177 3.262-3.904 5.54 0.671 5.619 7.144 4.902 11.409 4.829-0.105-0.388-0.21-0.776-0.315-1.165-3.56 2.636-8.58 11.381-0.562 12.174 2.34 0.231 4.247-0.259 6.423-1.142 0.883-0.358 1.698-0.845 2.525-1.311 0.775-0.437 1.976-2.122 2.008-0.692 0.166 7.357-0.865 14.714-1.194 22.056-0.036 0.804 1.214 0.801 1.25-2e-3z" fill="#4469d8" stroke="#fdffff" stroke-linejoin="round" stroke-width="5.2162"/>
     </g>
     <g fill="#4469d8" stroke="#fdffff" stroke-width="5.2162">
      <path d="m22.301 83.156c-0.441-6.032-1.072-12.618 0.266-18.564 0.138-0.613-0.578-1.042-1.045-0.608-1.743 1.625-3.443 2.831-5.732 3.604-6.34-3.393-7.913-6.373-4.717-8.939 0.988-0.856 2.034-1.633 3.139-2.329 0.287-0.191 0.397-0.544 0.225-0.855-0.658-1.178-1.392-2.163-2.251-3.191-4.397-5.264-0.382-9.414 4.759-10.875 0.271-0.077 0.455-0.322 0.459-0.603 0.036-2.864 0.313-5.642 1.094-8.407 1.865-6.606 10.255-9.181 13.143-1.487 0.28 0.748 1.489 0.424 1.205-0.332-2.517-6.706-9.574-7.649-13.918-2.003-2.305 2.996-2.61 7.466-2.759 11.084-0.035 0.85-3.839 2.269-4.496 2.694-1.034 0.669-2.219 2.098-2.45 3.312-0.808 4.233 1.103 6.056 3.512 9.323 0.405 0.548-5.327 5.252-5.317 7.279 0.016 3.468 2.455 5.64 5.605 6.645 3.404 1.086 7.127-1.932 9.386-4.037-0.349-0.203-0.697-0.405-1.045-0.608-1.368 6.079-0.762 12.734-0.311 18.896 0.056 0.8 1.306 0.806 1.248 1e-3z" fill="#4469d8" stroke="#fdffff" stroke-linejoin="round" stroke-width="5.2162"/>
     </g>
     <g fill="#4469d8" stroke="#fdffff" stroke-width="5.2162">
      <path d="m21.424 64.741c1.983 2.707 4.981 4.199 8.349 3.637 3.594-0.6 5.191-4.13 5.291-7.411 0.024-0.807-1.226-0.804-1.25 0-0.202 6.67-7.523 8.313-11.31 3.143-0.472-0.643-1.557-0.02-1.08 0.631z" fill="#4469d8" stroke="#fdffff" stroke-width="5.2162"/>
     </g>
     <g fill="#4469d8" stroke="#fdffff" stroke-width="5.2162">
      <path d="m74.661 80.878c2.869-5.406 3.251-12.191 2.679-18.182-0.036-0.381-0.375-0.742-0.791-0.603-1.482 0.496-9.677 1.84-5.634-4.557 0.251-0.397-0.075-0.952-0.54-0.94-4.913 0.123-9.233-0.937-9.57-6.683-0.047-0.801-1.297-0.806-1.25 0 0.201 3.426 1.375 5.828 4.622 7.214 1.514 0.646 3.278 0.7 4.894 0.751-0.658-0.021-0.338 3.074-0.216 3.489 0.625 2.13 4.101 2.773 5.896 2.466 2.606-0.446 1.551 3.288 1.477 5.177-0.15 3.833-0.832 7.82-2.646 11.236-0.378 0.713 0.701 1.345 1.079 0.632z" fill="#4469d8" stroke="#fdffff" stroke-width="5.2162"/>
     </g>
     <g fill="#4469d8" stroke="#fdffff" stroke-width="5.2162">
      <path d="m76.881 63.299c3.341-0.618 7.425-1.372 7.423-5.67 0-1.473-0.141-3.462-1.403-4.486 0.524 0.425 2.703-1.287 3.381-1.885 5.097-4.499 1.607-12.585-4.301-13.85-0.222-0.047 2.216-4.5 2.515-5.157 0.832-1.834 0.614-3.634-8e-3 -5.472-1.133-3.347-6.327-9.06-10.153-9.283-1.411-0.082-2.449-0.077-3.515 0.881-1.212 1.09 0.842 3.98-1.963 2.484-4.82-2.573-5.125 2.25-7.856 4.852-0.584 0.557 0.301 1.439 0.885 0.884 1.199-1.143 0.961-0.736 1.574-2.026 2.202-4.641 4.768-2.589 7.178-1.388 0.334 0.167 0.839 0.047 0.918-0.374 0.208-1.098 0.205-1.025 0.186-2.169 2.787-1.84 5.084-1.596 6.891 0.731 0.745 0.715 1.449 1.469 2.113 2.261 4.874 5.507 2.097 8.833-0.535 13.968-0.228 0.445 0.06 0.897 0.54 0.94 8.368 0.749 8.684 11.983 0.698 13.757-0.432 0.096-0.64 0.75-0.276 1.044 4.99 4.046-0.386 7.969-4.622 8.753-0.794 0.147-0.458 1.351 0.33 1.205z" fill="#4469d8" stroke="#fdffff" stroke-linejoin="round" stroke-width="5.2162"/>
     </g>
    </g>
   </g>
  </g>
 </g>
</svg>
\">"
      ],
      "metadata": {
        "id": "jSS2pwvk9D1m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Binary classification\n",
        "\n",
        "The purpose of this Python notebook is to use investigate using SAT to learn a classifier that takes an input data point and establishes whether it belongs to one of two clases.  We'll learn the classifier using a training dataset consisting of data points with known labels.\n",
        "\n",
        "You should have completed the notebook on SAT constructions before attempting this notebook.  \n",
        "\n",
        "You can save a local copy of this notebook in your Google account and work through it in Colab (recommended) or you can download the notebook and run it locally using Jupyter notebook or similar.\n",
        "\n",
        "Contact me at iclimbtreesmail@gmail.com if you find any mistakes or have any suggestions."
      ],
      "metadata": {
        "id": "h61RTzt_9KE1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.preprocessing import scale\n",
        "!pip install --only-binary=:all: z3-solver\n",
        "from z3 import *"
      ],
      "metadata": {
        "id": "13LPQcDi94Hz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start by creating some random data. We'll create 60 points and assign each to one of two random classes.  We'll keep the data in the $2\\times 60$ matrix $X$ and the class labels in the length 60 vector $y$."
      ],
      "metadata": {
        "id": "1JgXBNWz_cOD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_data = 60;\n",
        "X, y = make_blobs(n_samples=n_data, centers=4, random_state=0, cluster_std=1.0)\n",
        "# Put the data points in the columns\n",
        "X = X.T\n",
        "# Combine clusters\n",
        "y = y % 2\n",
        "# Scale to reasonable values\n",
        "X = scale(X, axis=1) * 1.3\n",
        "print(\"Data matrix size: (%d x %d)\"%(X.shape[0],X.shape[1]))\n",
        "print(\"Label vector size: %d\"%(y.shape[0]))"
      ],
      "metadata": {
        "id": "o1yPpE6E-Nqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's plot this data"
      ],
      "metadata": {
        "id": "A0rq2uZ2_r2k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig,ax = plt.subplots(1,1,figsize=(5,5))\n",
        "for i in range(n_data):\n",
        "    if y[i] == 0:\n",
        "        ax.scatter(X[0, i], X[1, i], marker='s', s=50, c='#c29b83') # Square for class 0\n",
        "    else:\n",
        "        ax.scatter(X[0, i], X[1, i], marker='o', s=50, c='#a0d9d4') # Circle for class 1\n",
        "ax.set_xlabel('x1');\n",
        "ax.set_ylabel('x2');\n",
        "ax.set_ylim([-3,3])\n",
        "ax.set_xlim([-3,3])\n",
        "ax.set_aspect('equal')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UyVWlFN7_urP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our goal wil be to learn a rule that (as much as possible) that takes and input data points and assigns the label 0 to the data if it is in the first class and the label 1 if it is in the second class.\n",
        "\n",
        "We're going to formulate this as a SAT problem, so we need to deal with the fact that each 2D data point is based on continuous values.  To this end, we convert each data point $\\mathbf{x}\\in\\mathbb{R}^2$ into a binary feature $\\mathbf{x}\\in \\mathbb{R}^{N}$.\n",
        "\n",
        "We'll do this by generating $N$ random lines in the 2D space and assigning a binary feature for a point depending on which side of the line it is on."
      ],
      "metadata": {
        "id": "4XQi5eur_wWO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set seed so we get same result each time\n",
        "np.random.seed(2)\n",
        "\n",
        "# Number of random lines\n",
        "n_features = 50\n",
        "\n",
        "# Storage for the line params\n",
        "line_params = np.zeros((3, n_features))\n",
        "# Storage for the features themselves\n",
        "features = np.zeros((n_features, n_data))\n",
        "\n",
        "# Generate random lines and assign binary features\n",
        "for c_feature in range(n_features):\n",
        "    # Generate two random points to define a line\n",
        "    point1 = np.random.uniform(low=-3, high=3, size=2)\n",
        "    point2 = np.random.uniform(low=-3, high=3, size=2)\n",
        "\n",
        "    # Calculate the line equation (ax + by + c = 0)\n",
        "    a = point2[1] - point1[1]\n",
        "    b = point1[0] - point2[0]\n",
        "    c = -a * point1[0] - b * point1[1]\n",
        "\n",
        "    # Store the line\n",
        "    line_params[:, c_feature] = [a, b, c]\n",
        "\n",
        "    # Assign binary features based on which side of the line each point is\n",
        "    for c_data in range(n_data):\n",
        "        if a * X[0, c_data] + b * X[1, c_data] + c > 0:\n",
        "            features[c_feature, c_data] = 1\n",
        "\n",
        "print(\"Feature matrix size: (%d x %d)\"%(features.shape[0],features.shape[1]))"
      ],
      "metadata": {
        "id": "B92Dem09VWl9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll now learn a single layer neural network to classify the data as well as possible.\n",
        "\n",
        "We'll use this routine to draw the final result"
      ],
      "metadata": {
        "id": "oDr4q5tFpeGq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def draw_decision_boundary(X,y,predict,predict_params):\n",
        "\n",
        "  # Define a grid of values to plot\n",
        "  x1_plot_vals = np.linspace(-3,3,400)\n",
        "  x2_plot_vals = np.linspace(-3,3,400)\n",
        "  x1_plot, x2_plot = np.meshgrid(x1_plot_vals, x2_plot_vals)\n",
        "\n",
        "  # Make a prediction for each of these values\n",
        "  y_plot = predict(x1_plot, x2_plot, predict_params)\n",
        "\n",
        "  # Draw the points and the decision boundary\n",
        "  fig,ax = plt.subplots(1,1,figsize=(5,5))\n",
        "  plt.contourf(x1_plot_vals, x2_plot_vals, y_plot, cmap=plt.cm.RdBu, alpha=0.8)\n",
        "  for i in range(n_data):\n",
        "    if y[i] == 0:\n",
        "        ax.scatter(X[0, i], X[1, i], marker='o', s=50, c='#c29b83') # Circle for class 0\n",
        "    else:\n",
        "        ax.scatter(X[0, i], X[1, i], marker='s', s=50, c='#a0d9d4') # Square for class 1\n",
        "  ax.set_xlabel('x1');   ax.set_ylabel('x2');\n",
        "  ax.set_ylim([-3,3]);  ax.set_xlim([-3,3])\n",
        "  ax.set_aspect('equal')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "H8u7kA32VxGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SAT constructions that we will need\n",
        "\n",
        "# Takes a list of z3.Bool variables and returns constraints\n",
        "# ensuring that there is exactly one true\n",
        "def exactly_one(x):\n",
        "  return PbEq([(i,1) for i in x],1)\n",
        "\n",
        "# Takes a list of z3.Bool variables and returns constraints\n",
        "# ensuring that are at least k true\n",
        "def at_least_k(x,k):\n",
        "  return PbGe([(i,1) for i in x],k)"
      ],
      "metadata": {
        "id": "mhySZULUPpHW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the main routine that runs the SAT solver.  It takes the binary features, true labels $y$ and the minimum number of correct answers required $k$.  \n",
        "\n",
        "It sets up the binary variables used in the SAT formulation and it returns their values if the solver returns SAT."
      ],
      "metadata": {
        "id": "P_STq8aJgoLq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fit_multiple_feature_model(features, y, k):\n",
        "\n",
        "  # Find number of features and data\n",
        "  n_features = features.shape[0]\n",
        "  n_data = features.shape[1]\n",
        "\n",
        "  # These variables will identify which features are used\n",
        "  features_used = [ z3.Bool(\"feat_used_{%d}\"%(c_feature)) for c_feature in range(0,n_features)]\n",
        "  # These variable will identify where each feature should be flipped\n",
        "  features_flipped = [ z3.Bool(\"feat_flipped_{%d}\"%(c_feature)) for c_feature in range(0,n_features)]\n",
        "  # These variables will indicate the classification of the points based on each feature\n",
        "  preactivations = [[ z3.Bool(\"preact_{%d,%d}\"%(c_feature, c_data)) for c_data in range(0,n_data)]for c_feature in range(0,n_features)]\n",
        "  # These variables will indicate the final classification of the points.\n",
        "  y_est = [ z3.Bool(\"y_est_{%d}\"%(c_data)) for c_data in range(0,n_data)]\n",
        "  # These variables will indicate if the point is classified correctly\n",
        "  y_correct = [ z3.Bool(\"y_correct_{%d}\"%(c_data)) for c_data in range(0,n_data)]\n",
        "\n",
        "  # Set up the SAT solver\n",
        "  s = Solver()\n",
        "\n",
        "  # Add the constraints\n",
        "  s = add_multiple_feature_constraints(s, features_used, features_flipped, y_correct, y_est, preactivations, k, features)\n",
        "\n",
        "  # Run the SAT solver\n",
        "  sat_result = s.check()\n",
        "\n",
        "  # If it is then draw crossword, otherwise return\n",
        "  if sat_result == z3.sat:\n",
        "      result = s.model()\n",
        "      # Retrieve the final variables\n",
        "      features_used_out = np.array([int(bool(result[z3.Bool(\"feat_used_{%d}\"%(c_feature))])) for c_feature in range(0,n_features)])\n",
        "      features_flipped_out = np.array([int(bool(result[z3.Bool(\"feat_flipped_{%d}\"%(c_feature))])) for c_feature in range(0,n_features)])\n",
        "      preactivations_out = np.array([[int(bool(result[z3.Bool(\"preact_{%d,%d}\"%(c_feature,c_data))]))  for c_data in range(0,n_data)]for c_feature in range(0,n_features)] )\n",
        "      y_est_out = np.array([int(bool(result[z3.Bool(\"y_est_{%d}\"%(c_data))])) for c_data in range(0,n_data)])\n",
        "      y_correct_out = np.array([int(bool(result[z3.Bool(\"y_correct_{%d}\"%(c_data))])) for c_data in range(0,n_data)])\n",
        "      return sat_result, features_used_out, features_flipped_out, y_est_out, preactivations_out, y_correct_out\n",
        "  else:\n",
        "      return sat_result, None, None, None, None, None"
      ],
      "metadata": {
        "id": "HKI3E5FKhwFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_multiple_feature_constraints(s, features_used, features_flipped, y_correct, y_est, preactivations, k, features):\n",
        "\n",
        "  # Find number of features and data\n",
        "  n_features = features.shape[0]\n",
        "  n_data = features.shape[1]\n",
        "\n",
        "  # Constraint 1:  For each preactivation (term to be ANDed together)\n",
        "  #                       either (i) the feature is not used (i.e., \\overline{\\phi}_k) and we return true whatever the input was or\n",
        "  #                       or     (ii) the feature is used (i.e., \\phi_k)  and we return a value that depends on the input and whether the feature was flipped.\n",
        "  #                                      If the feature was not flipped (i.e., \\overline{\\theta}_k) we return the input z_j\n",
        "  #                                      If it was flipped, we return its complement \\overline{z}_j.\n",
        "  for c_data in range(n_data):\n",
        "    for c_feature in range(n_features):\n",
        "        s.add(preactivations[c_feature][c_data]== Or(Not(features_used[c_feature]), And(features_used[c_feature],features_flipped[c_feature]==Not(bool(features[c_feature,c_data])))))\n",
        "\n",
        "  # Constraint 2:  y_est is logical AND of all estimates from individual features\n",
        "  for c_data in range(n_data):\n",
        "    s.add(y_est[c_data] == And([preactivations[c_feature][c_data] for c_feature in range(n_features)]))\n",
        "\n",
        "  # Constraint 3:  For each data point the data is correct if y_est matches the test label\n",
        "  for c_data in range(0,n_data):\n",
        "    s.add(y_correct[c_data] == (y_est[c_data] == bool(y[c_data])))\n",
        "\n",
        "  # Constraint 4:  We require at least k features to be correct\n",
        "  s.add(at_least_k(y_correct, k))\n",
        "\n",
        "  return s"
      ],
      "metadata": {
        "id": "qwZmRz55mAx-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sat_result, features_used_out, features_flipped_out, y_est_out, preactivations_out, y_correct_out = fit_multiple_feature_model(features, y, k=45)\n",
        "\n",
        "if sat_result == z3.sat:\n",
        "  print(\"SAT\")\n",
        "  print(\"\\nFeatures used:\\n\", features_used_out)\n",
        "  print(\"\\nFeatures flipped:\\n\", features_flipped_out)\n",
        "  print(\"\\nPreactivations \\n\",preactivations_out)\n",
        "  print(\"\\nY estimated (1st row) vs y_true (second row)\\n\",np.concatenate((y_est_out.reshape(1,n_data),y.reshape(1,n_data)),axis=0))\n",
        "  print(\"\\nCorrectly classified:\\n\", y_correct_out)\n",
        "else:\n",
        "  print(\"UNSAT\")"
      ],
      "metadata": {
        "id": "iQez3B02nEAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_multiple_features(x1,x2,params):\n",
        "  features_used = params[\"features_used\"]\n",
        "  feature_flipped = params[\"features_flipped\"]\n",
        "  line_params = params[\"line_params\"]\n",
        "  result = np.full(x1.shape, True, dtype=bool)\n",
        "  n_features = len(features_used)\n",
        "  for c_feature in range(n_features):\n",
        "    if features_used[c_feature]:\n",
        "      if feature_flipped[c_feature]:\n",
        "        result = np.logical_and(result,line_params[0,c_feature] * x1 + line_params[1,c_feature] * x2 + line_params[2,c_feature] < 0)\n",
        "      else:\n",
        "        result = np.logical_and(result,line_params[0,c_feature] * x1 + line_params[1,c_feature] * x2 + line_params[2,c_feature] > 0)\n",
        "  return result"
      ],
      "metadata": {
        "id": "JU4A_rItnON0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_params ={\"line_params\":line_params,\"features_used\":features_used_out,\"features_flipped\":features_flipped_out}\n",
        "draw_decision_boundary(X,y,predict_multiple_features, predict_params)"
      ],
      "metadata": {
        "id": "lvEpNDUWvmRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you did this correctly, it should be able to classify 45/60 points correctly.  However, AND-ing together regions created by lines like this is guaranteed to produce a convex region for the blue points in class 1 (a convex region is one where any two points on the boundary can be joined with straight line without intersecting the boundary).\n",
        "\n",
        "This limits the expressivity of the classifier.  A simple way to make progress is to compute several of these regions and logically OR them together to create a more complex region.  \n",
        "\n",
        "In the terminology of neural networks, the computations for the individual regions are stored in the \"activations\" of \"hidden variables\"."
      ],
      "metadata": {
        "id": "eauWDz_lxSpO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fit_multilayer_model(features, y, k, n_hidden):\n",
        "\n",
        "  # Find number of features and data\n",
        "  n_features = features.shape[0]\n",
        "  n_data = features.shape[1]\n",
        "\n",
        "  # These variables will identify which features are used\n",
        "  features_used = [[ z3.Bool(\"feat_used_{%d,%d}\"%(c_feature, c_hidden)) for c_hidden in range(0, n_hidden) ] for c_feature in range(0,n_features)]\n",
        "  # These variable will identify where each feature should be flipped\n",
        "  features_flipped = [[ z3.Bool(\"feat_flipped_{%d,%d}\"%(c_feature, c_hidden)) for c_hidden in range(0, n_hidden) ] for c_feature in range(0,n_features)]\n",
        "  # These variables will indicate the classification of the points based on each feature\n",
        "  preactivations = [[[ z3.Bool(\"preact_{%d,%d,%d}\"%(c_feature, c_data, c_hidden)) for c_hidden in range(0,n_hidden)] for c_data in range(0,n_data)]for c_feature in range(0,n_features)]\n",
        "  # These variables will be the values at the hidden units (created by AND-ing together the relevant preactivations)\n",
        "  activations = [[ z3.Bool(\"act_{%d,%d}\"%(c_data, c_hidden)) for c_hidden in range(0,n_hidden)] for c_data in range(0,n_data)]\n",
        "\n",
        "  # These variables will indicate the final classification of the points.\n",
        "  y_est = [ z3.Bool(\"y_est_{%d}\"%(c_data)) for c_data in range(0,n_data)]\n",
        "  # These variables will indicate if the point is classified correctly\n",
        "  y_correct = [ z3.Bool(\"y_correct_{%d}\"%(c_data)) for c_data in range(0,n_data)]\n",
        "\n",
        "  # Set up the SAT solver\n",
        "  s = Solver()\n",
        "\n",
        "  # Add the constraints\n",
        "  s = add_multilayer_constraints(s, features_used, features_flipped, y_correct, y_est, preactivations, activations, k, n_hidden, features)\n",
        "\n",
        "  # Run the SAT solver\n",
        "  sat_result = s.check()\n",
        "\n",
        "  # If it is then draw crossword, otherwise return\n",
        "  if sat_result == z3.sat:\n",
        "      result = s.model()\n",
        "      # Retrieve the final variables\n",
        "      features_used_out = np.array([[int(bool(result[z3.Bool(\"feat_used_{%d,%d}\"%(c_feature, c_hidden))])) for c_hidden in range(0, n_hidden) ] for c_feature in range(0,n_features)])\n",
        "      features_flipped_out = np.array([[int(bool(result[z3.Bool(\"feat_flipped_{%d,%d}\"%(c_feature,c_hidden))])) for c_hidden in range(0, n_hidden) ]for c_feature in range(0,n_features)])\n",
        "      preactivations_out = np.array([[[int(bool(result[z3.Bool(\"preact_{%d,%d}\"%(c_feature,c_data))])) for c_hidden in range(0, n_hidden) ] for c_data in range(0,n_data)]for c_feature in range(0,n_features)] )\n",
        "      activations_out = np.array([[ int(bool(result[z3.Bool(\"act_{%d,%d}\"%(c_data, c_hidden))])) for c_hidden in range(0,n_hidden)] for c_data in range(0,n_data)])\n",
        "      y_est_out = np.array([int(bool(result[z3.Bool(\"y_est_{%d}\"%(c_data))])) for c_data in range(0,n_data)])\n",
        "      y_correct_out = np.array([int(bool(result[z3.Bool(\"y_correct_{%d}\"%(c_data))])) for c_data in range(0,n_data)])\n",
        "      return sat_result, features_used_out, features_flipped_out, y_est_out, preactivations_out, activations_out, y_correct_out\n",
        "  else:\n",
        "      return sat_result, None, None, None, None, None, None"
      ],
      "metadata": {
        "id": "ZHwoY4OpyWwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_multilayer_constraints(s, features_used, features_flipped, y_correct, y_est, preactivations, activations, k, n_hidden, features):\n",
        "\n",
        "  # Find number of features and data\n",
        "  n_features = features.shape[0]\n",
        "  n_data = features.shape[1]\n",
        "\n",
        "  # Constraint 1:  For each preactivation (term to be ANDed together)\n",
        "  #                       either (i) the feature is not used (i.e., \\overline{\\phi}_k) and we return true whatever the input was or\n",
        "  #                       or     (ii) the feature is used (i.e., \\phi_k)  and we return a value that depends on the input and whether the feature was flipped.\n",
        "  #                                      If the feature was not flipped (i.e., \\overline{\\theta}_k) we return the input z_j\n",
        "  #                                      If it was flipped, we return its complement \\overline{z}_j.\n",
        "  for c_data in range(n_data):\n",
        "    for c_feature in range(n_features):\n",
        "      for c_hidden in range (n_hidden):\n",
        "        s.add(preactivations[c_feature][c_data][c_hidden]== Or(Not(features_used[c_feature][c_hidden]), And(features_used[c_feature][c_hidden],features_flipped[c_feature][c_hidden]==Not(bool(features[c_feature,c_data])))))\n",
        "\n",
        "  # Constraint 2:  For each hidden layer activation is logical AND of all of the preactivations\n",
        "  for c_data in range(n_data):\n",
        "    for c_hidden in range(n_hidden):\n",
        "      s.add(activations[c_data][c_hidden] == And([preactivations[c_feature][c_data][c_hidden] for c_feature in range(n_features)]))\n",
        "\n",
        "  # Constraint 3:  Estimated value is logical OR of all activaitons\n",
        "  for c_data in range(n_data):\n",
        "    s.add(y_est[c_data] == Or([activations[c_data][c_hidden] for c_hidden in range(n_hidden)]))\n",
        "\n",
        "  # Constraint 5:  For each data point the data is correct if y_est matches the test label\n",
        "  for c_data in range(0,n_data):\n",
        "    s.add(y_correct[c_data] == (y_est[c_data] == bool(y[c_data])))\n",
        "\n",
        "  # Constraint 6:  We require at least k features to be correct\n",
        "  s.add(at_least_k(y_correct, k))\n",
        "\n",
        "  return s"
      ],
      "metadata": {
        "id": "1ayMBQkgzxc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sat_result, features_used_out, features_flipped_out, y_est_out, preactivations_out, activations_out, y_correct_out = \\\n",
        "              fit_multilayer_model(features, y, k=60, n_hidden=3)\n",
        "\n",
        "if sat_result == z3.sat:\n",
        "  print(\"SAT\")\n",
        "  print(\"\\nFeatures used hidden unit 1:\\n\", features_used_out[:,0])\n",
        "  print(\"\\nFeatures flipped hidden unit 1:\\n\", features_flipped_out[:,0])\n",
        "  print(\"\\nPreactivations hidden unit 1\\n\",preactivations_out[:,:,0])\n",
        "\n",
        "  print(\"\\nFeatures used hidden unit 2:\\n\", features_used_out[:,1])\n",
        "  print(\"\\nFeatures flipped hidden unit 2:\\n\", features_flipped_out[:,1])\n",
        "  print(\"\\nPreactivations hidden unit 2\\n\",preactivations_out[:,:,1])\n",
        "\n",
        "  print(\"\\nActivations\\n\",activations_out.T)\n",
        "\n",
        "  print(\"\\nY estimated (1st row) vs y_true (second row)\\n\",np.concatenate((y_est_out.reshape(1,n_data),y.reshape(1,n_data)),axis=0))\n",
        "  print(\"\\nCorrectly classified:\\n\", y_correct_out)\n",
        "else:\n",
        "  print(\"UNSAT\")"
      ],
      "metadata": {
        "id": "dCgXSNgP2-8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_multilayer(x1,x2,params):\n",
        "  features_used = params[\"features_used\"]\n",
        "  feature_flipped = params[\"features_flipped\"]\n",
        "  line_params = params[\"line_params\"]\n",
        "  n_features = len(features_used)\n",
        "  n_hidden = features_used.shape[1]\n",
        "\n",
        "  prediction = np.full(x1.shape, False, dtype=bool)\n",
        "  for c_hidden in range(n_hidden):\n",
        "    activation = np.full(x1.shape, True, dtype=bool)\n",
        "    for c_feature in range(n_features):\n",
        "      if features_used[c_feature,c_hidden]:\n",
        "        if feature_flipped[c_feature,c_hidden]:\n",
        "          activation = np.logical_and(activation,line_params[0,c_feature] * x1 + line_params[1,c_feature] * x2 + line_params[2,c_feature] < 0)\n",
        "        else:\n",
        "          activation = np.logical_and(activation,line_params[0,c_feature] * x1 + line_params[1,c_feature] * x2 + line_params[2,c_feature] > 0)\n",
        "    prediction = np.logical_or(prediction, activation)\n",
        "\n",
        "  return prediction"
      ],
      "metadata": {
        "id": "gvpc65rb5jYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_params ={\"line_params\":line_params,\"features_used\":features_used_out,\"features_flipped\":features_flipped_out}\n",
        "draw_decision_boundary(X,y,predict_multilayer, predict_params)"
      ],
      "metadata": {
        "id": "osHqVWfT6pIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you have done this correctly, you should find that you can now classify all 60 points correctly."
      ],
      "metadata": {
        "id": "CD1ATUuYsYC7"
      }
    }
  ]
}